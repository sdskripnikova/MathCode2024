{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca05c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class BaseSmoothOracle(object):\n",
    "    \"\"\"\n",
    "    Base class for implementation of oracles.\n",
    "    \"\"\"\n",
    "    def func(self, x):\n",
    "        \"\"\"\n",
    "        Computes the value of function at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Func oracle is not implemented.')\n",
    "\n",
    "    def grad(self, x):\n",
    "        \"\"\"\n",
    "        Computes the gradient at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Grad oracle is not implemented.')\n",
    "    \n",
    "    def hess(self, x):\n",
    "        \"\"\"\n",
    "        Computes the Hessian matrix at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Hessian oracle is not implemented.')\n",
    "    \n",
    "    def func_directional(self, x, d, alpha):\n",
    "        \"\"\"\n",
    "        Computes phi(alpha) = f(x + alpha*d).\n",
    "        \"\"\"\n",
    "        return np.squeeze(self.func(x + alpha * d))\n",
    "\n",
    "    def grad_directional(self, x, d, alpha):\n",
    "        \"\"\"\n",
    "        Computes phi'(alpha) = (f(x + alpha*d))'_{alpha}\n",
    "        \"\"\"\n",
    "        return np.squeeze(self.grad(x + alpha * d).dot(d))\n",
    "\n",
    "\n",
    "class QuadraticOracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for quadratic function:\n",
    "       func(x) = 1/2 x^TAx - b^Tx.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A, b):\n",
    "        if not scipy.sparse.isspmatrix_dia(A) and not np.allclose(A, A.T):\n",
    "            raise ValueError('A should be a symmetric matrix.')\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "\n",
    "    def func(self, x):\n",
    "        return 0.5 * np.dot(self.A.dot(x), x) - self.b.dot(x)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.A.dot(x) - self.b\n",
    "\n",
    "    def hess(self, x):\n",
    "        return self.A \n",
    "\n",
    "\n",
    "class LogRegL2Oracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for logistic regression with l2 regularization:\n",
    "         func(x) = 1/m sum_i log(1 + exp(-b_i * a_i^T x)) + regcoef / 2 ||x||_2^2.\n",
    "\n",
    "    Let A and b be parameters of the logistic regression (feature matrix\n",
    "    and labels vector respectively).\n",
    "    For user-friendly interface use create_log_reg_oracle()\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        matvec_Ax : function\n",
    "            Computes matrix-vector product Ax, where x is a vector of size n.\n",
    "        matvec_ATx : function of x\n",
    "            Computes matrix-vector product A^Tx, where x is a vector of size m.\n",
    "        matmat_ATsA : function\n",
    "            Computes matrix-matrix-matrix product A^T * Diag(s) * A,\n",
    "    \"\"\"\n",
    "    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n",
    "        self.matvec_Ax = matvec_Ax\n",
    "        self.matvec_ATx = matvec_ATx\n",
    "        self.matmat_ATsA = matmat_ATsA\n",
    "        self.b = b\n",
    "        self.regcoef = regcoef\n",
    "        self.m = b.shape[0]\n",
    "\n",
    "    def func(self, x):\n",
    "        cross_ent = (1/self.m) * (np.logaddexp(0, -self.b * (self.matvec_Ax(x))) @ np.ones(self.m))\n",
    "        return cross_ent + (self.regcoef/2)*np.linalg.norm(x, ord=2)**2\n",
    "\n",
    "    def grad(self, x):\n",
    "        return -(1 / self.m) * self.matvec_ATx(self.b * expit(-self.b*self.matvec_Ax(x))) + \\\n",
    "                self.regcoef * x\n",
    "\n",
    "    def hess(self, x):\n",
    "        S = np.diag(expit(-self.b*self.matvec_Ax(x)))\n",
    "\n",
    "        return (1 / self.m) * self.matmat_ATsA(S @ (np.eye(self.m) - S)) + self.regcoef*np.eye(x.shape[0])\n",
    "\n",
    "\n",
    "class LogRegL2OptimizedOracle(LogRegL2Oracle):\n",
    "    \"\"\"\n",
    "    Oracle for logistic regression with l2 regularization\n",
    "    with optimized *_directional methods (are used in line_search).\n",
    "\n",
    "    For explanation see LogRegL2Oracle.\n",
    "    \"\"\"\n",
    "    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n",
    "        super().__init__(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n",
    "\n",
    "    def func_directional(self, x, d, alpha):\n",
    "        return self.func(x + alpha*d)\n",
    "\n",
    "    def grad_directional(self, x, d, alpha):\n",
    "        return self.grad(x + alpha*d).T @ d\n",
    "\n",
    "\n",
    "def create_log_reg_oracle(A, b, regcoef, oracle_type='usual'):\n",
    "    \"\"\"\n",
    "    Auxiliary function for creating logistic regression oracles.\n",
    "        `oracle_type` must be either 'usual' or 'optimized'\n",
    "    \"\"\"\n",
    "    matvec_Ax = lambda x: A @ x\n",
    "    matvec_ATx = lambda x: A.T @ x\n",
    "\n",
    "    def matmat_ATsA(s):\n",
    "        return A.T.dot(s @ A)\n",
    "\n",
    "    if oracle_type == 'usual':\n",
    "        oracle = LogRegL2Oracle\n",
    "    elif oracle_type == 'optimized':\n",
    "        oracle = LogRegL2OptimizedOracle\n",
    "    else:\n",
    "        raise 'Unknown oracle_type=%s' % oracle_type\n",
    "    return oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n",
    "\n",
    "\n",
    "\n",
    "def grad_finite_diff(func, x, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns approximation of the gradient using finite differences:\n",
    "        result_i := (f(x + eps * e_i) - f(x)) / eps,\n",
    "        where e_i are coordinate vectors:\n",
    "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
    "                          >> i <<\n",
    "    \"\"\"\n",
    "    grad = (func(np.diag(x + eps)) - func(np.diag(x))) / eps\n",
    "    return grad\n",
    "\n",
    "\n",
    "def hess_finite_diff(func, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Returns approximation of the Hessian using finite differences:\n",
    "        result_{ij} := (f(x + eps * e_i + eps * e_j)\n",
    "                               - f(x + eps * e_i) \n",
    "                               - f(x + eps * e_j)\n",
    "                               + f(x)) / eps^2,\n",
    "        where e_i are coordinate vectors:\n",
    "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
    "                          >> i <<\n",
    "    \"\"\"\n",
    "    H = np.zeros((x.shape[0], x.shape[0]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            e_i = np.zeros_like(x)\n",
    "            e_j = np.zeros_like(x)\n",
    "            e_i[i] = 1\n",
    "            e_j[j] = 1\n",
    "            H[i, j] = (func(x + eps * e_i + eps * e_j) - func(x + eps * e_i)\n",
    "                       - func(x + eps * e_j) + func(x))\n",
    "    return H / eps**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ca366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
