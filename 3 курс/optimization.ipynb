{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70cea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "import scipy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class LineSearchTool(object):\n",
    "    \"\"\"\n",
    "    Line search tool for adaptively tuning the step size of the algorithm.\n",
    "\n",
    "    method : String containing 'Wolfe', 'Armijo' or 'Constant'\n",
    "        Method of tuning step-size.\n",
    "        Must be be one of the following strings:\n",
    "            - 'Wolfe' -- enforce strong Wolfe conditions;\n",
    "            - 'Armijo\" -- adaptive Armijo rule;\n",
    "            - 'Constant' -- constant step size.\n",
    "    kwargs :\n",
    "        Additional parameters of line_search method:\n",
    "\n",
    "        If method == 'Wolfe':\n",
    "            c1, c2 : Constants for strong Wolfe conditions\n",
    "            alpha_0 : Starting point for the backtracking procedure\n",
    "                to be used in Armijo method in case of failure of Wolfe method.\n",
    "        If method == 'Armijo':\n",
    "            c1 : Constant for Armijo rule\n",
    "            alpha_0 : Starting point for the backtracking procedure.\n",
    "        If method == 'Constant':\n",
    "            c : The step size which is returned on every step.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='Wolfe', **kwargs):\n",
    "        self._method = method\n",
    "        if self._method == 'Wolfe':\n",
    "            self.c1 = kwargs.get('c1', 1e-4)\n",
    "            self.c2 = kwargs.get('c2', 0.9)\n",
    "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
    "        elif self._method == 'Armijo':\n",
    "            self.c1 = kwargs.get('c1', 1e-4)\n",
    "            self.min_step_size = 1e-05\n",
    "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
    "        elif self._method == 'Constant':\n",
    "            self.c = kwargs.get('c', 1.0)\n",
    "        else:\n",
    "            raise ValueError('Unknown method {}'.format(method))\n",
    "            \n",
    "           \n",
    "    @classmethod\n",
    "    def from_dict(cls, options):\n",
    "        if type(options) != dict:\n",
    "            raise TypeError('LineSearchTool initializer must be of type dict')\n",
    "        return cls(**options)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def line_search(self, oracle, x_k, d_k, previous_alpha=None):\n",
    "        \"\"\"\n",
    "        Finds the step size alpha for a given starting point x_k\n",
    "        and for a given search direction d_k that satisfies necessary\n",
    "        conditions for phi(alpha) = oracle.func(x_k + alpha * d_k).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        oracle : BaseSmoothOracle-descendant object\n",
    "            Oracle with .func_directional() and .grad_directional() methods implemented for computing\n",
    "            function values and its directional derivatives.\n",
    "        x_k : np.array\n",
    "            Starting point\n",
    "        d_k : np.array\n",
    "            Search direction\n",
    "        previous_alpha : float or None\n",
    "            Starting point to use instead of self.alpha_0 to keep the progress from\n",
    "             previous steps. If None, self.alpha_0, is used as a starting point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        alpha : float or None if failure\n",
    "            Chosen step size\n",
    "        \"\"\"\n",
    "        if self._method == 'Constant':\n",
    "            return self.c\n",
    "        if self._method == 'Wolfe':\n",
    "            phi = lambda alpha: oracle.func_directional(x_k, d_k, alpha)\n",
    "            d_phi = lambda alpha: oracle.grad_directional(x_k, d_k, alpha)\n",
    "\n",
    "            ret = scipy.optimize.linesearch.scalar_search_wolfe2(phi, d_phi, c1=self.c1, c2=self.c2)[0]\n",
    "\n",
    "            if not (ret is None):\n",
    "                return ret\n",
    "            else:\n",
    "                self._method = 'Armijo'\n",
    "\n",
    "        if self._method == 'Armijo':\n",
    "            if previous_alpha is None:\n",
    "                alpha = self.alpha_0\n",
    "            else:\n",
    "                alpha = previous_alpha\n",
    "            phi0 = oracle.func(x_k)\n",
    "            while oracle.func_directional(x_k, d_k, alpha) > \\\n",
    "                    phi0 + self.c1 * alpha * oracle.grad_directional(x_k, d_k, 0):\n",
    "                alpha /= 2\n",
    "            return alpha\n",
    "        \n",
    "        \n",
    "def get_line_search_tool(line_search_options=None):\n",
    "    if line_search_options:\n",
    "        if type(line_search_options) is LineSearchTool:\n",
    "            return line_search_options\n",
    "        else:\n",
    "            return LineSearchTool.from_dict(line_search_options)\n",
    "    else:\n",
    "        return LineSearchTool()\n",
    "\n",
    "\n",
    "def gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000,\n",
    "                     line_search_options=None, trace=False, display=False):\n",
    "    history = defaultdict(list) if trace else None\n",
    "    line_search_tool = get_line_search_tool(line_search_options)\n",
    "    x_k = np.copy(x_0)\n",
    "\n",
    "    # alpha = line_search_tool.line_search(oracle, x_0, oracle.grad(x_0))\n",
    "    alpha = None\n",
    "    start = datetime.now()\n",
    "    if history is not None:\n",
    "        history['time'].append(datetime.now() - start)\n",
    "        history['func'].append(oracle.func(x_k))\n",
    "        history['grad_norm'].append(np.linalg.norm(oracle.grad(x_k)))\n",
    "        history['x'].append(x_k)\n",
    "    for i in range(max_iter):\n",
    "        alpha = line_search_tool.line_search(oracle, x_k, -oracle.grad(x_k), previous_alpha=alpha)\n",
    "        x_k1 = x_k - alpha * oracle.grad(x_k)\n",
    "        x_k = np.copy(x_k1)\n",
    "        if history is not None:\n",
    "            history['time'].append(datetime.now() - start)\n",
    "            history['func'].append(oracle.func(x_k))\n",
    "            history['grad_norm'].append(np.linalg.norm(oracle.grad(x_k)))\n",
    "            history['x'].append(x_k)\n",
    "        if np.linalg.norm(oracle.grad(x_k), ord=2) ** 2 <= tolerance * np.linalg.norm(oracle.grad(x_0), ord=2) ** 2:\n",
    "            return x_k, 'success', history\n",
    "        if display:\n",
    "            print('smt to display idk, here is x_k')\n",
    "            print(x_k)\n",
    "\n",
    "        # alpha = line_search_tool.line_search(oracle, x_k, oracle.grad(x_k), previous_alpha=alpha)\n",
    "\n",
    "    return x_k, \"iterations_exceeded\", history\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def newton(oracle, x_0, tolerance=1e-5, max_iter=100,\n",
    "           line_search_options=None, trace=False, display=False):\n",
    "    \"\"\"\n",
    "    Newton's optimization method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    oracle : BaseSmoothOracle-descendant object\n",
    "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
    "        function value, its gradient and Hessian respectively. If the Hessian\n",
    "        returned by the oracle is not positive-definite method stops with message=\"newton_direction_error\"\n",
    "    x_0 : np.array\n",
    "        Starting point for optimization algorithm\n",
    "    tolerance : float\n",
    "        Epsilon value for stopping criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "    line_search_options : dict, LineSearchTool or None\n",
    "        Dictionary with line search options. See LineSearchTool class for details.\n",
    "    trace : bool\n",
    "        If True, the progress information is appended into history dictionary during training.\n",
    "        Otherwise None is returned instead of history.\n",
    "    display : bool\n",
    "        If True, debug information is displayed during optimization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_star : np.array\n",
    "        The point found by the optimization procedure\n",
    "    message : string\n",
    "        'success' or the description of error:\n",
    "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
    "                the stopping criterion.\n",
    "            - 'newton_direction_error': in case of failure of solving linear system with Hessian matrix (e.g. non-invertible matrix).\n",
    "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
    "    history : dictionary of lists or None\n",
    "        Dictionary containing the progress information or None if trace=False.\n",
    "        Dictionary has to be organized as follows:\n",
    "            - history['time'] : list of floats, containing time passed from the start of the method\n",
    "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
    "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
    "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
    "    >> x_opt, message, history = newton(oracle, np.zeros(5), line_search_options={'method': 'Constant', 'c': 1.0})\n",
    "    >> print('Found optimal point: {}'.format(x_opt))\n",
    "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
    "    \"\"\"\n",
    "    history = defaultdict(list) if trace else None\n",
    "    start = datetime.now()\n",
    "\n",
    "    line_search_tool = get_line_search_tool(line_search_options)\n",
    "    x_k = np.copy(x_0)\n",
    "    alpha = None\n",
    "    if history is not None:\n",
    "        history['time'].append(datetime.now() - start)\n",
    "        history['func'].append(oracle.func(x_k))\n",
    "        history['grad_norm'].append(np.linalg.norm(oracle.grad(x_k)))\n",
    "        history['x'].append(x_k)\n",
    "    for i in range(max_iter):\n",
    "        alpha = line_search_tool.line_search(oracle, x_k, -oracle.grad(x_k), previous_alpha=1)\n",
    "        try:\n",
    "            H_cho = scipy.linalg.cho_factor(oracle.hess(x_k))\n",
    "            d_k = scipy.linalg.cho_solve(H_cho, -oracle.grad(x_k))\n",
    "            x_k1 = x_k + alpha * d_k\n",
    "            x_k = np.copy(x_k1)\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            if np.linalg.norm(oracle.grad(x_k), ord=2) ** 2 <= tolerance * np.linalg.norm(oracle.grad(x_0), ord=2) ** 2:\n",
    "                return x_k, 'success', history\n",
    "            return x_k, \"computational_error\", history\n",
    "        if history is not None:\n",
    "            history['time'].append(datetime.now() - start)\n",
    "            history['func'].append(oracle.func(x_k))\n",
    "            history['grad_norm'].append(np.linalg.norm(oracle.grad(x_k)))\n",
    "            history['x'].append(x_k)\n",
    "        if display:\n",
    "            print('smt to display idk')\n",
    "            print(x_k)\n",
    "        if np.linalg.norm(oracle.grad(x_k), ord=2) ** 2 <= tolerance * np.linalg.norm(oracle.grad(x_0), ord=2) ** 2:\n",
    "            return x_k, 'success', history\n",
    "\n",
    "    return x_k, \"iterations_exceeded\", history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b195e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
